{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc02de2a",
   "metadata": {},
   "source": [
    "# Lab Three: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be95ce89",
   "metadata": {},
   "source": [
    "Matt Farrow, Amber Clark, Blake Freeman, Megan Ball"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92470e4b",
   "metadata": {},
   "source": [
    "## **2015 Flight Delays and Cancellations**\n",
    "Data Source: [Kaggle](https://www.kaggle.com/usdot/flight-delays?select=flights.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a82361d",
   "metadata": {},
   "source": [
    "Our data set consists of over 5 million rows of flight information in the domestic United States for the year of 2015. In order to optimize our modeling time, we have narrowed the scope of our classification tasks to the Dallas area only (Dallas Love Field and DFW airports). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96115ee",
   "metadata": {},
   "source": [
    "## Rubric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df04ee12",
   "metadata": {},
   "source": [
    "### [Business Understanding](#Business-Understanding) (10 points total)\n",
    "\n",
    "- [10 points] Describe the purpose of the data set you selected (i.e., why was this data collected in the first place?). How will you measure the effectiveness of a good algorithm? Why does your chosen validation method make sense for this specific dataset and the stakeholders needs?\n",
    "\n",
    "### [Data Understanding](#Data-Understanding) (20 points total)\n",
    "\n",
    "- [10 points] Describe the meaning and type of data (scale, values, etc.) for each attribute in the data file. Verify data quality: Are there missing values? Duplicate data? Outliers? Are those mistakes? How do you deal with these problems?\n",
    "- [10 points] Visualize the any important attributes appropriately. Important: Provide an interpretation for any charts or graphs.\n",
    "\n",
    "### [Modeling and Evaluation](#Modeling-and-Evaluation) (50 points total)\n",
    "\n",
    "Different tasks will require different evaluation methods. Be as thorough as possible when analyzing the data you have chosen and use visualizations of the results to explain the performance and expected outcomes whenever possible. Guide the reader through your analysis with plenty of discussion of the results.\n",
    "\n",
    "#### Option A: Cluster Analysis\n",
    "\n",
    "- Perform cluster analysis using several clustering methods\n",
    "- How did you determine a suitable number of clusters for each method?\n",
    "- Use internal and/or external validation measures to describe and compare the clusterings and the clusters (some visual methods would be good).\n",
    "- Describe your results. What findings are the most interesting and why?\n",
    "\n",
    "### [Deployment](#Deployment) (10 points total)\n",
    "\n",
    "Be critical of your performance and tell the reader how you current model might be usable by other parties. Did you achieve your goals? If not, can you reign in the utility of your modeling?\n",
    "\n",
    "- How useful is your model for interested parties (i.e., the companies or organizations that might want to use it)?\n",
    "- How would your deploy your model for interested parties?\n",
    "- What other data should be collected?\n",
    "- How often would the model need to be updated, etc.?\n",
    "\n",
    "### [Exceptional Work](#Exceptional-Work) (10 points total)\n",
    "\n",
    "You have free reign to provide additional analyses or combine analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e5d45e",
   "metadata": {},
   "source": [
    "# Business Understanding\n",
    "Jump to [top](#Rubric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eac0415",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "Jump to [top](#Rubric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972de0ca",
   "metadata": {},
   "source": [
    "The initial data pre-processing has already been covered in Labs 1, 2, and the Mini-Lab. Here we have collapsed our code as much as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f629c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#from datetime import datetime\n",
    "import altair as alt\n",
    "import datetime\n",
    "\n",
    "# Machine learning\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a66eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Due to the way these columns are formatted, we want to keep the leading zeros during our import. \n",
    "# Later on will convert them to a time format.\n",
    "dtype_t = {'SCHEDULED_DEPARTURE': str,\n",
    "           'DEPARTURE_TIME': str,\n",
    "           'WHEELS_OFF': str,\n",
    "           'SCHEDULED_TIME': str,\n",
    "           'WHEELS_ON': str,\n",
    "           'SCHEDULED_ARRIVAL': str,\n",
    "           'ARRIVAL_TIME': str\n",
    "          }\n",
    "\n",
    "# Read in the data directly\n",
    "airlines = pd.read_csv('../Data/airlines.csv')\n",
    "airports = pd.read_csv('../Data/airports.csv')\n",
    "flights  = pd.read_csv('../Data/flights.csv', dtype = dtype_t)\n",
    "\n",
    "# Read in the data directly from GitHub\n",
    "# airlines = pd.read_csv('https://raw.githubusercontent.com/mattfarrow1/7331-machine-learning-1/main/Data/airlines.csv')\n",
    "# airports = pd.read_csv('https://raw.githubusercontent.com/mattfarrow1/7331-machine-learning-1/main/Data/airports.csv')\n",
    "# flights  = pd.read_csv('https://media.githubusercontent.com/media/mattfarrow1/7331-machine-learning-1/main/Data/flights.csv', dtype = dtype_t)\n",
    "\n",
    "# Rename columns in preparation for merge\n",
    "airlines.rename(columns={'IATA_CODE': 'AIRLINE_CODE'}, inplace=True)\n",
    "flights.rename(columns={'AIRLINE': 'AIRLINE_CODE'}, inplace=True)\n",
    "\n",
    "# Merge data together\n",
    "df = pd.merge(flights, airlines, on='AIRLINE_CODE', how = 'left')\n",
    "\n",
    "# Subset to DFW Area\n",
    "df = df[(df.ORIGIN_AIRPORT == 'DFW') | (df.ORIGIN_AIRPORT == 'DAL')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d81b894",
   "metadata": {},
   "source": [
    "### Create New Variables\n",
    "\n",
    "The data contains several timestamp and continuous variables that add additional complexity, and several models cannot run with timestamp data. In order to simplify the data, we created buckets to analyze similar attributes together.\n",
    "\n",
    "We first convert `SCHEDULED_DEPARTURE`, `DEPARTURE_TIME`, `ARRIVAL_TIME`, and `SCHEDULED_ARRIVAL` into buckets based on their timestamp.\n",
    "\n",
    "    Overnight: 12:00am - 5:59am\n",
    "    Morning: 6:00am - 11:59am\n",
    "    Afternoon: 12:00pm - 5:59pm\n",
    "    Evening: 6:00pm - 11:59pm\n",
    "\n",
    "[Flight] `DISTANCE` is also divided into buckets.\n",
    "\n",
    "    Short: 1-99 miles\n",
    "    Medium: 100-999 miles\n",
    "    Long: 1,000+ miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da81f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert times into buckets for morning, afternoon, and evening as most models cannot handle timestamps.\n",
    "cut_labels = ['overnight', 'morning', 'afternoon', 'evening']\n",
    "cut_bins = [0, 600, 1200, 1800, 2359]\n",
    "\n",
    "df['SCHED_DEPARTURE_TIME'] = pd.cut(df['SCHEDULED_DEPARTURE'].astype(float), \n",
    "                                    bins=cut_bins, \n",
    "                                    labels=cut_labels, \n",
    "                                    include_lowest=True)\n",
    "df['ACTUAL_DEPARTURE_TIME'] = pd.cut(df['DEPARTURE_TIME'].astype(float), \n",
    "                                     bins=cut_bins, \n",
    "                                     labels=cut_labels, \n",
    "                                     include_lowest=True)\n",
    "df['SCHED_ARRIVAL_TIME'] = pd.cut(df['SCHEDULED_ARRIVAL'].astype(float), \n",
    "                                  bins=cut_bins, \n",
    "                                  labels=cut_labels, \n",
    "                                  include_lowest=True)\n",
    "df['ACTUAL_ARRIVAL_TIME'] = pd.cut(df['ARRIVAL_TIME'].astype(float), \n",
    "                                  bins=cut_bins, \n",
    "                                  labels=cut_labels, \n",
    "                                  include_lowest=True)\n",
    "\n",
    "# Bucket Flight Distance\n",
    "distance_labels = ['Short', 'Medium', 'Long']\n",
    "distance_bins   = [1, 100, 1000, np.inf]\n",
    "df['DISTANCE_BUCKET'] = pd.cut(df['DISTANCE'],\n",
    "                               bins=distance_bins,\n",
    "                               labels=distance_labels)\n",
    "\n",
    "# Create a new column where the arrival_delay > 0 means it's delayed(=1) and if <= 0 it's not delayed(=0)\n",
    "get_delay = lambda x: 0 if x <= 0 else 1\n",
    "df['DELAYED'] = df.ARRIVAL_DELAY.apply(get_delay)\n",
    "\n",
    "# Look at our data with the buckets\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c990d61a",
   "metadata": {},
   "source": [
    "### Process Dates & Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e97560e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://stackoverflow.com/questions/54487059/pandas-how-to-create-a-single-date-column-from-columns-containing-year-month\n",
    "df['FLIGHT_DATE'] = pd.to_datetime([f'{y}-{m}-{d}' for y, m, d in zip(df.YEAR, df.MONTH, df.DAY)])\n",
    "\n",
    "# Creating a function to change the way of representation of time in the column\n",
    "def fun_format_time(hours):\n",
    "        if hours == 2400:\n",
    "            hours = 0\n",
    "        else:\n",
    "            hours = \"{0:04d}\".format(int(hours))\n",
    "            Hourmin = datetime.time(int(hours[0:2]), int(hours[2:4]))\n",
    "            return Hourmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c277af62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the time columns\n",
    "cols = [\"SCHEDULED_DEPARTURE\", \n",
    "        \"DEPARTURE_TIME\", \n",
    "        \"SCHEDULED_ARRIVAL\", \n",
    "        \"ARRIVAL_TIME\",\n",
    "        \"WHEELS_ON\",\n",
    "        \"WHEELS_OFF\"]\n",
    "\n",
    "# Convert times to float in order to correctly process them through the function\n",
    "df[cols] = df[cols].astype(float)\n",
    "\n",
    "# Run times through the new function\n",
    "# Code adapted from: https://stackoverflow.com/questions/35232705/how-to-test-for-nans-in-an-apply-function-in-pandas\n",
    "df['SCHEDULED_DEPARTURE'] = df['SCHEDULED_DEPARTURE'].apply(lambda x: fun_format_time(x) if pd.notnull(x) else x)\n",
    "df['DEPARTURE_TIME']      = df['DEPARTURE_TIME'].apply(lambda x: fun_format_time(x) if pd.notnull(x) else x)\n",
    "df['SCHEDULED_ARRIVAL']   = df['SCHEDULED_ARRIVAL'].apply(lambda x: fun_format_time(x) if pd.notnull(x) else x)\n",
    "df['ARRIVAL_TIME']        = df['ARRIVAL_TIME'].apply(lambda x: fun_format_time(x) if pd.notnull(x) else x)\n",
    "df['WHEELS_ON']           = df['WHEELS_ON'].apply(lambda x: fun_format_time(x) if pd.notnull(x) else x)\n",
    "df['WHEELS_OFF']          = df['WHEELS_OFF'].apply(lambda x: fun_format_time(x) if pd.notnull(x) else x)\n",
    "\n",
    "# Combine date & time for departure and arrival\n",
    "# Source: https://stackoverflow.com/questions/17978092/combine-date-and-time-columns-using-python-pandas\n",
    "df['SCHEDULED_DEPARTURE_DT'] = pd.to_datetime(df['FLIGHT_DATE'].astype(str) + ' ' + df['SCHEDULED_DEPARTURE'].astype(str))\n",
    "df['SCHEDULED_ARRIVAL_DT']   = pd.to_datetime(df['FLIGHT_DATE'].astype(str) + ' ' + df['SCHEDULED_ARRIVAL'].astype(str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb3ff6e",
   "metadata": {},
   "source": [
    "### Append Dallas-Area Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4b8ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "import datetime\n",
    "weather = pd.read_csv('../Data/dfw_weather.csv')\n",
    "weather['dt_iso'] = weather['dt_iso'].astype(str)\n",
    "\n",
    "# Remove \"+0000 UTC\"\n",
    "weather['dt_iso_update'] = weather['dt_iso'].str.split('+').str[0]\n",
    "\n",
    "# Convert new column to a datetime type\n",
    "weather['date_time'] =  pd.to_datetime(weather['dt_iso_update'], format='%Y-%m-%d %H:%M')\n",
    "\n",
    "weather['date_time'] = weather['date_time'].dt.round('30min')  \n",
    "df['SCHEDULED_DEPARTURE_DT'] = df['SCHEDULED_DEPARTURE_DT'].dt.round('30min')\n",
    "\n",
    "df = pd.merge(df, weather, left_on='SCHEDULED_DEPARTURE_DT', right_on='date_time')\n",
    "\n",
    "# Remove unnecessary columns from weather data\n",
    "col_to_drop = ['dt', 'dt_iso', 'timezone', 'city_name', 'lat', 'lon', 'feels_like', 'temp_min', 'temp_max',\n",
    "              'sea_level', 'grnd_level', 'dt_iso_update', 'weather_icon', 'weather_description', 'date_time']\n",
    "df = df.drop(columns = col_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0edcf0b",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7978c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-critical columns WHEELS_ON and WHEELS_OFF\n",
    "df = df.drop(['WHEELS_ON','WHEELS_OFF'], axis=1)\n",
    "\n",
    "# Add category\n",
    "df['ACTUAL_DEPARTURE_TIME'] = df['ACTUAL_DEPARTURE_TIME'].cat.add_categories(['N'])\n",
    "df['ACTUAL_ARRIVAL_TIME'] = df['ACTUAL_ARRIVAL_TIME'].cat.add_categories(['N'])\n",
    "\n",
    "# Fill missing values with 'N' for 'N/A'\n",
    "df['ACTUAL_DEPARTURE_TIME'] = df['ACTUAL_DEPARTURE_TIME'].fillna('N')\n",
    "df['ACTUAL_ARRIVAL_TIME'] = df['ACTUAL_ARRIVAL_TIME'].fillna('N')\n",
    "\n",
    "# Convert missing values to 'N' for 'N/A'\n",
    "df['CANCELLATION_REASON'] = df['CANCELLATION_REASON'].fillna('N')\n",
    "\n",
    "# Update missing values in times to 0. \n",
    "# Will be updating times to a binary (1 = yes action happened, 0 = no action happened)\n",
    "df['DEPARTURE_TIME'] = df['DEPARTURE_TIME'].fillna(0)\n",
    "\n",
    "# Change all non-null values to 1\n",
    "df.loc[(df.DEPARTURE_TIME != '0'), 'DEPARTURE_TIME'] = 1\n",
    "\n",
    "# Change column name to 'DEPARTED'\n",
    "df.rename(columns={'DEPARTURE_TIME': 'DEPARTED'}, inplace=True)\n",
    "\n",
    "# Update remaining columns using same logic\n",
    "cols = ['ARRIVAL_TIME']\n",
    "df[cols] = df[cols].fillna(0)\n",
    "df.loc[(df.ARRIVAL_TIME != '0'), 'ARRIVAL_TIME'] = 1\n",
    "df.rename(columns={'ARRIVAL_TIME': 'ARRIVED'}, inplace=True)\n",
    "\n",
    "# Fill missing values with 0\n",
    "cols = ['AIR_SYSTEM_DELAY','SECURITY_DELAY','AIRLINE_DELAY','LATE_AIRCRAFT_DELAY','WEATHER_DELAY', \n",
    "       'rain_1h', 'rain_3h', 'snow_1h', 'snow_3h']\n",
    "df[cols] = df[cols].fillna(0)\n",
    "\n",
    "# Change remaining null values to 0 if flight was cancelled\n",
    "df.loc[(df.CANCELLED == 1), ('DEPARTURE_DELAY', 'TAXI_OUT', 'ELAPSED_TIME','AIR_TIME','TAXI_IN','ARRIVAL_DELAY')] = 0\n",
    "\n",
    "# Drop remaining missing values and check total cancels left\n",
    "df = df.dropna()\n",
    "\n",
    "# Delete date columns ahead of modeling\n",
    "df = df.drop(columns = ['FLIGHT_DATE', 'SCHEDULED_DEPARTURE_DT', 'SCHEDULED_ARRIVAL_DT'])\n",
    "\n",
    "# Convert back to string\n",
    "df.SCHEDULED_DEPARTURE = df.SCHEDULED_DEPARTURE.astype(str)\n",
    "df.SCHEDULED_ARRIVAL = df.SCHEDULED_ARRIVAL.astype(str)\n",
    "\n",
    "# Remove colons\n",
    "df.SCHEDULED_DEPARTURE = df.SCHEDULED_DEPARTURE.str.replace(r'\\D+', '')\n",
    "df.SCHEDULED_ARRIVAL = df.SCHEDULED_ARRIVAL.str.replace(r'\\D+', '')\n",
    "\n",
    "# Convert to float\n",
    "df.SCHEDULED_DEPARTURE = df.SCHEDULED_DEPARTURE.astype(int)\n",
    "df.SCHEDULED_ARRIVAL = df.SCHEDULED_ARRIVAL.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b3f6cf",
   "metadata": {},
   "source": [
    "### Log Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed70b1f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min DEPARTURE_DELAY -24.0\n",
      "Min ARRIVAL_DELAY -56.0\n",
      "Min DISTANCE 89\n",
      "Min TAXI_IN 0.0\n",
      "Min ELAPSED_TIME 0.0\n",
      "Min AIR_TIME 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Min DEPARTURE_DELAY\", min(df[\"DEPARTURE_DELAY\"]))\n",
    "print(\"Min ARRIVAL_DELAY\", min(df[\"ARRIVAL_DELAY\"]))\n",
    "print(\"Min DISTANCE\", min(df[\"DISTANCE\"]))\n",
    "print(\"Min TAXI_IN\", min(df[\"TAXI_IN\"]))\n",
    "print(\"Min ELAPSED_TIME\", min(df[\"ELAPSED_TIME\"]))\n",
    "print(\"Min AIR_TIME\", min(df[\"AIR_TIME\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "285c0c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log transformation keeping the 0 in the data sets. Because we have negative values, need to offset to make minimum\n",
    "# equal to zero and not a negative number. For the other vars, no need to run lambda function as min > 0 which improves\n",
    "# run time\n",
    "df[\"DEPARTURE_DELAY_log\"] = df[\"DEPARTURE_DELAY\"].map(lambda i: np.log(i + 24) if i != -24 else 0) \n",
    "df[\"ARRIVAL_DELAY_log\"]   = df[\"ARRIVAL_DELAY\"].map(lambda i: np.log(i + 56) if i != -56 else 0)\n",
    "df[\"DISTANCE_log\"]        = np.log(df[\"DISTANCE\"])\n",
    "df[\"TAXI_IN_log\"]         = np.log1p(df[\"TAXI_IN\"])\n",
    "df[\"ELAPSED_TIME_log\"]    = np.log1p(df[\"ELAPSED_TIME\"])\n",
    "df[\"AIR_TIME_log\"]        = np.log1p(df[\"AIR_TIME\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2aac8a",
   "metadata": {},
   "source": [
    "### Feature Removals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fd3754",
   "metadata": {},
   "source": [
    "Here we remove redundant columns to further reduce the data size. Columns that are being removed:\n",
    "\n",
    "- `YEAR`: All rows are from 2015, no need to include this.\n",
    "- `AIRLINE`: We have AIRLINE_CODE which is the same information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bddd704a",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop1 = ['YEAR','AIRLINE']\n",
    "df = df.drop(columns = col_to_drop1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813a7719",
   "metadata": {},
   "source": [
    "### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21622cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N906EV    786\n",
      "N902EV    772\n",
      "N905EV    769\n",
      "N904EV    757\n",
      "N903EV    752\n",
      "         ... \n",
      "N443UA      5\n",
      "N16713      5\n",
      "N360NW      5\n",
      "N7830A      5\n",
      "N12924      5\n",
      "Name: TAIL_NUMBER, Length: 2660, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Filter out instances where a tail number appears less than 5 times\n",
    "df = df[df.groupby('TAIL_NUMBER').TAIL_NUMBER.transform(len) > 4]\n",
    "\n",
    "# Encode Destination Airport & Tail Number\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "df['DESTINATION_AIRPORT_encode'] = labelencoder.fit_transform(df['DESTINATION_AIRPORT'])\n",
    "df.dropna(subset = [\"DESTINATION_AIRPORT_encode\"], inplace=True)\n",
    "df['TAIL_NUMBER_encode'] = labelencoder.fit_transform(df['TAIL_NUMBER'])\n",
    "\n",
    "# Drop original columns\n",
    "col_to_drop2 = ['TAIL_NUMBER','DESTINATION_AIRPORT']\n",
    "df = df.drop(columns = col_to_drop2)\n",
    "\n",
    "# One-hot encode categorical columns\n",
    "categorical_columns = ['AIRLINE_CODE', 'CANCELLATION_REASON', 'SCHED_DEPARTURE_TIME', \n",
    "                       'ACTUAL_DEPARTURE_TIME','SCHED_ARRIVAL_TIME', 'ACTUAL_ARRIVAL_TIME',\n",
    "                       'DISTANCE_BUCKET', 'weather_main', 'ORIGIN_AIRPORT']\n",
    "\n",
    "for column in categorical_columns:\n",
    "  tempdf = pd.get_dummies(df[categorical_columns], prefix = categorical_columns, drop_first = True)\n",
    "  df_OHE = pd.merge(\n",
    "      left = df,\n",
    "      right = tempdf,\n",
    "      left_index=True,\n",
    "      right_index=True\n",
    "  )\n",
    "  df_OHE = df_OHE.drop(columns = categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8cf3a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduled time needs to be int\n",
    "df_OHE['SCHEDULED_TIME'] = df_OHE['SCHEDULED_TIME'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16162c0b",
   "metadata": {},
   "source": [
    "### Flight Delay Response Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4381afad",
   "metadata": {},
   "source": [
    "Here, we create our second classification response variable for the departure delay. We want to predict which departure delay group a flight is expected to be in based on the information and data we have.\n",
    "\n",
    "We group the data in the following way:\n",
    "- EARLY is defined as `0` and is any value where the `DEPARTURE_DELAY` is < 0.\n",
    "- ON-TIME is defined as `1` and is any value where 0 <= `DEPARTURE_DELAY` <= 10\n",
    "- LATE is defined as `2` and is any value where 11 <= `DEPARTURE_DELAY` <= 30\n",
    "- VERY LATE is defined as `3` and is any value where 31 <= `DEPARTURE_DELAY` <= 60\n",
    "- EXTREMELY LATE is defined as `4` and is any value where `DEPARTURE_DELAY` >= 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b85d0bd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    99535\n",
       "1    50033\n",
       "2    24784\n",
       "3    13854\n",
       "4    13030\n",
       "Name: DELAY_BUCKET, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add response variable bucket for delay time for departure\n",
    "# 0 is Early (negative time)\n",
    "# 1 is On_Time or between 0 and 10 minutes late\n",
    "# 2 is Late (between 11 and 30 min late)\n",
    "# 3 is very late (between 31 and 60 min late)\n",
    "# 4 is extremely late (over 61 min late)\n",
    "\n",
    "delay_labels = ['0', '1', '2', '3', '4']\n",
    "delay_bins   = [-np.inf, -1, 10, 30, 60, np.inf]\n",
    "df_OHE['DELAY_BUCKET'] = pd.cut(df_OHE['DEPARTURE_DELAY'],\n",
    "                               bins=delay_bins,\n",
    "                               labels=delay_labels)\n",
    "\n",
    "#check counts by bucket\n",
    "df_OHE['DELAY_BUCKET'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a82864ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from category to int\n",
    "df_OHE['DELAY_BUCKET'] = df_OHE['DELAY_BUCKET'].astype(int)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "col_to_drop3 = ['DEPARTURE_DELAY', 'ARRIVAL_DELAY', 'DISTANCE', 'TAXI_IN', 'ELAPSED_TIME', 'AIR_TIME']\n",
    "df_OHE = df_OHE.drop(columns = col_to_drop3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a522c283",
   "metadata": {},
   "source": [
    "## 1.9 Delay and Cancel Data set creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3cdb46a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of our data for each new set\n",
    "df_delay = df_OHE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2280ca72",
   "metadata": {},
   "source": [
    "### 1.9.1 Delay Correlations/Feature Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a54dbc",
   "metadata": {},
   "source": [
    "We can remove several variables for our delay bucket group. We will filter out any cancelled flights, as these are not delayed. Then we can also remove `CANCELLED` and our `CANCELLATION_REASON` encoded columns. Because we don't know whether or not the flight will be delayed prior to the analysis, we will also remove `ACTUAL_ARRIVAL_TIME` and `ACTUAL_DEPARTURE_TIME` and all of of our other delay time related columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0303e881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out cancelled flights\n",
    "df_delay = df_delay[df_delay.CANCELLED == 0]\n",
    "\n",
    "col_to_drop4 = ['CANCELLED', \n",
    "                'CANCELLATION_REASON_B', \n",
    "                'CANCELLATION_REASON_C', \n",
    "                'CANCELLATION_REASON_N', \n",
    "                'ACTUAL_DEPARTURE_TIME_morning', \n",
    "                'ACTUAL_DEPARTURE_TIME_afternoon', \n",
    "                'ACTUAL_DEPARTURE_TIME_evening',\n",
    "                'ACTUAL_DEPARTURE_TIME_N',\n",
    "                'ACTUAL_ARRIVAL_TIME_morning',\n",
    "                'ACTUAL_ARRIVAL_TIME_afternoon',\n",
    "                'SCHEDULED_DEPARTURE',\n",
    "                'SCHEDULED_ARRIVAL',\n",
    "                'AIR_SYSTEM_DELAY',\n",
    "                'SECURITY_DELAY', \n",
    "                'ACTUAL_ARRIVAL_TIME_evening',\n",
    "                'ACTUAL_ARRIVAL_TIME_N',\n",
    "                'AIRLINE_DELAY', \n",
    "                'LATE_AIRCRAFT_DELAY', \n",
    "                'WEATHER_DELAY', \n",
    "                'DELAYED', \n",
    "                'DEPARTURE_DELAY_log',\n",
    "                'ARRIVAL_DELAY_log', \n",
    "                'ELAPSED_TIME_log', \n",
    "                'DEPARTED', \n",
    "                'ARRIVED',\n",
    "                'TAXI_IN_log',\n",
    "                'AIR_TIME_log']\n",
    "\n",
    "df_delay = df_delay.drop(columns = col_to_drop4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "76ddbe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine correlation visually using Seaborn. \n",
    "# (Code adapted from 02. Data Visualization.ipynb)\n",
    "\n",
    "# cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# sns.set(style = \"darkgrid\") # one of the many styles to plot using\n",
    "\n",
    "# f, ax = plt.subplots(figsize = (20, 20))\n",
    "\n",
    "# # Create heatmap\n",
    "# sns.heatmap(df_delay.corr(), cmap=cmap, annot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1113184",
   "metadata": {},
   "source": [
    "We have too many features for a correlation matrix plot to be interpretable. Let's print out the variables that have a correlation greater than 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4b673ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DISTANCE_log', 'DISTANCE_BUCKET_Long', 'ORIGIN_AIRPORT_DFW']\n"
     ]
    }
   ],
   "source": [
    "# Create correlation matrix with absolute values only\n",
    "corr_matrix_abs = df_delay.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix_abs.where(np.triu(np.ones(corr_matrix_abs.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find features with correlation greater than 0.8\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fdbaa04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop7 = ['DISTANCE_log', 'DIVERTED']\n",
    "\n",
    "df_delay = df_delay.drop(columns = col_to_drop7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d979672",
   "metadata": {},
   "source": [
    "We decided to drop `DISTANCE_log` and `DIVERTED` from our data set. This is because `DISTANCE_log` is correlated to `DISTANCE_BUCKET_Long` and `DIVERTED` has no relevance to the departure delay time. We do not remove `ORIGIN_AIRPORT_DFW` as it is deemed an important variable for our modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd16cff1",
   "metadata": {},
   "source": [
    "The delay data set contains basic flight information from our original data plus weather data for the appropriate date & time of each flight, encoded variables for `DESTINATION_AIRPORT` and `TAIL_NUMBER`, and one-hot encoded airline codes.\n",
    "Newly created variables included buckets for the flightâ€™s scheduled departure and arrival times (morning, afternoon, and evening), distance (medium and long), and a response variable `DELAY_BUCKET` that groups delay times by length of delay in minutes.\n",
    "- **Early** is defined as 0 and is any value where the `DEPARTURE_DELAY` is < 0.\n",
    "- **On-Time** is defined as 1 and is any value where 0 <= `DEPARTURE_DELAY` <= 10\n",
    "- **Late** is defined as 2 and is any value where 11 <= `DEPARTURE_DELAY` <= 30\n",
    "- **Very Late** is defined as 3 and is any value where 31 <= `DEPARTURE_DELAY` <= 60\n",
    "- **Extremely Late** is defined as 4 and is any value where `DEPARTURE_DELAY` >= 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c6db5b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save data\n",
    "#df_cancel.to_csv('../Data/df_cancel.csv', index=False)\n",
    "#df_delay.to_csv('../Data/df_delay.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c33ed95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Load data from here to save time\n",
    "# df_cancel = pd.read_csv('../Data/df_cancel.csv')\n",
    "# df_delay = pd.read_csv('../Data/df_delay.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f171a2",
   "metadata": {},
   "source": [
    "# Modeling and Evaluation\n",
    "Jump to [top](#Rubric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557b90f7",
   "metadata": {},
   "source": [
    "Next we will split our delay data, but first, let's look at the distribution of the buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d1a49c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    50.899245\n",
       "1    22.679274\n",
       "2    12.673802\n",
       "3     7.084524\n",
       "4     6.663155\n",
       "Name: DELAY_BUCKET, dtype: float64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Percentages of each delay bucket\n",
    "df_delay['DELAY_BUCKET'].value_counts() / len(df_delay) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a3078515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(195553, 50)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_delay.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fb82412a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    99535\n",
       "1    44350\n",
       "2    24784\n",
       "3    13854\n",
       "4    13030\n",
       "Name: DELAY_BUCKET, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_delay['DELAY_BUCKET'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34f28e",
   "metadata": {},
   "source": [
    "It appears that half of our data in bucket `0` reflects flights that departed early (negative delay value in the data set). This was a surprisingly high value. How did the rest of the buckets break down?\n",
    "\n",
    "- 22.7% departed on-time or between 1 and 10 minutes late\n",
    "- 12.7% departed late (between 11 and 30 min)\n",
    "- 7.1% departed very late (between 31 and 60 min)\n",
    "- 6.7% departed extremely late (over 61 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a9f03df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y for delay data set\n",
    "if 'DELAY_BUCKET' in df_delay:\n",
    "    y_del = df_delay['DELAY_BUCKET'].values\n",
    "    X_del = df_delay.iloc[:,:-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "816524c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(195553, 49)\n",
      "(195553,)\n"
     ]
    }
   ],
   "source": [
    "print(X_del.shape)\n",
    "print(y_del.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa03f2b",
   "metadata": {},
   "source": [
    "#### 4.2.1 Undersampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292e701c",
   "metadata": {},
   "source": [
    "Due to our class imbalance, we will create a data set that undersamples based on our majority class which is the `0` bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fb83a541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/\n",
    "\n",
    "# Define undersample strategy\n",
    "undersample = RandomUnderSampler(sampling_strategy = 'majority', random_state = 42)\n",
    "\n",
    "# Fit and apply the transform\n",
    "X_del_under, y_del_under = undersample.fit_resample(X_del, y_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c89b664b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109048, 49)\n",
      "(109048,)\n"
     ]
    }
   ],
   "source": [
    "print(X_del_under.shape)\n",
    "print(y_del_under.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "98d2dc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 44350, 2: 24784, 3: 13854, 0: 13030, 4: 13030})\n"
     ]
    }
   ],
   "source": [
    "# Summarize the new class distribution\n",
    "counter = Counter(y_del_under)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae85e5c3",
   "metadata": {},
   "source": [
    "After undersampling, our classes broke down as follows:\n",
    "\n",
    "- Class 0: 13,030\n",
    "- Class 1: 33,546\n",
    "- Class 2: 24,784 \n",
    "- Class 3: 13,854\n",
    "- Class 4: 13,030"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efb3eba",
   "metadata": {},
   "source": [
    "#### 4.2.2 Oversampling Using SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504e710f",
   "metadata": {},
   "source": [
    "We know that our delay data set is relatively balanced when it comes to flights that departed early vs. all the other groups together. Since the data between the classes is imbalanced, we'll transform the data using SMOTE (**S**ynthetic **M**inority **O**versampling **TE**chnique) and see if that returns better performance. This code is adapted from [Machine Learning Mastery](https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e28fd57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample using SMOTE\n",
    "oversample = SMOTE()\n",
    "X_del_smote, y_del_smote = oversample.fit_resample(X_del, y_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bf882127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(497675, 49)\n",
      "(497675,)\n"
     ]
    }
   ],
   "source": [
    "print(X_del_smote.shape)\n",
    "print(y_del_smote.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8bcf842d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({4: 99535, 1: 99535, 0: 99535, 2: 99535, 3: 99535})\n"
     ]
    }
   ],
   "source": [
    "# Summarize the new class distribution\n",
    "counter = Counter(y_del_smote)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a0621",
   "metadata": {},
   "source": [
    "Using SMOTE, all of our classes now have 110,339 observations in them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2669969",
   "metadata": {},
   "source": [
    "#### 4.2.3 Stratified Shuffle Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1012577d",
   "metadata": {},
   "source": [
    "Below we will perform Stratified Shuffle Split to create three train/test splits for modelling:\n",
    "1. Split on the original data\n",
    "2. Split on undersampled data\n",
    "3. Split on SMOTE/oversampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d330f124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on Original Data:\n",
      "\n",
      "Training Features Shape: (156442, 49)\n",
      "Training Labels Shape: (156442,)\n",
      "Testing Features Shape: (39111, 49)\n",
      "Testing Labels Shape: (39111,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss=StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42) \n",
    "\n",
    "for train_index, test_index in sss.split(X_del, y_del):\n",
    "    X_train_del, X_test_del = X_del[train_index], X_del[test_index]\n",
    "    y_train_del, y_test_del = y_del[train_index], y_del[test_index]\n",
    "\n",
    "print(\"Split on Original Data:\\n\")\n",
    "print('Training Features Shape:', X_train_del.shape)\n",
    "print('Training Labels Shape:', y_train_del.shape)\n",
    "print('Testing Features Shape:', X_test_del.shape)\n",
    "print('Testing Labels Shape:', y_test_del.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "88068fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on Undersampled Data:\n",
      "\n",
      "Training Features Shape: (87238, 49)\n",
      "Training Labels Shape: (87238,)\n",
      "Testing Features Shape: (21810, 49)\n",
      "Testing Labels Shape: (21810,)\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in sss.split(X_del_under, y_del_under):\n",
    "    X_train_del_under, X_test_del_under = X_del_under[train_index], X_del_under[test_index]\n",
    "    y_train_del_under, y_test_del_under = y_del_under[train_index], y_del_under[test_index]\n",
    "\n",
    "print(\"Split on Undersampled Data:\\n\")\n",
    "print('Training Features Shape:', X_train_del_under.shape)\n",
    "print('Training Labels Shape:', y_train_del_under.shape)\n",
    "print('Testing Features Shape:', X_test_del_under.shape)\n",
    "print('Testing Labels Shape:', y_test_del_under.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f3952866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split on Oversampled Data:\n",
      "\n",
      "Training Features Shape: (398140, 49)\n",
      "Training Labels Shape: (398140,)\n",
      "Testing Features Shape: (99535, 49)\n",
      "Testing Labels Shape: (99535,)\n"
     ]
    }
   ],
   "source": [
    "for train_index, test_index in sss.split(X_del_smote, y_del_smote):\n",
    "    X_train_del_smote, X_test_del_smote = X_del_smote[train_index], X_del_smote[test_index]\n",
    "    y_train_del_smote, y_test_del_smote = y_del_smote[train_index], y_del_smote[test_index]\n",
    "\n",
    "print(\"Split on Oversampled Data:\\n\")\n",
    "print('Training Features Shape:', X_train_del_smote.shape)\n",
    "print('Training Labels Shape:', y_train_del_smote.shape)\n",
    "print('Testing Features Shape:', X_test_del_smote.shape)\n",
    "print('Testing Labels Shape:', y_test_del_smote.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafd4430",
   "metadata": {},
   "source": [
    "### 5.2.1 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc79dd41",
   "metadata": {},
   "source": [
    "For delay modeling, we will first run random forest models on all three of our splits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01bcdb3",
   "metadata": {},
   "source": [
    "#### Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "84916d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestClassifier(n_estimators = 1000, \n",
    "                            n_jobs = -1,\n",
    "                            random_state = 42)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(X_train_del, y_train_del)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bef9763",
   "metadata": {},
   "source": [
    "Now that our model has been trained, we'll calculate predictions on our testing data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d71449b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[18051  1443   273    67    73]\n",
      " [ 5988  2345   336   103    98]\n",
      " [ 2870   903   941   117   126]\n",
      " [ 1459   383   228   530   171]\n",
      " [ 1208   268   188   116   826]]\n",
      "\n",
      "Accuracy: 0.58\n",
      "\n",
      "Micro Precision: 0.58\n",
      "Micro Recall: 0.58\n",
      "Micro F1-score: 0.58\n",
      "\n",
      "Macro Precision: 0.55\n",
      "Macro Recall: 0.37\n",
      "Macro F1-score: 0.41\n",
      "\n",
      "Weighted Precision: 0.55\n",
      "Weighted Recall: 0.58\n",
      "Weighted F1-score: 0.53\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.61      0.91      0.73     19907\n",
      "     Class 1       0.44      0.26      0.33      8870\n",
      "     Class 2       0.48      0.19      0.27      4957\n",
      "     Class 3       0.57      0.19      0.29      2771\n",
      "     Class 4       0.64      0.32      0.42      2606\n",
      "\n",
      "    accuracy                           0.58     39111\n",
      "   macro avg       0.55      0.37      0.41     39111\n",
      "weighted avg       0.55      0.58      0.53     39111\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Source: https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
    "\n",
    "# Importing confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "y_pred_del = rf.predict(X_test_del)\n",
    "\n",
    "# Define confusion matrix\n",
    "confusion = confusion_matrix(y_test_del, y_pred_del)\n",
    "\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "# Importing accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_del, y_pred_del)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_test_del, y_pred_del, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_test_del, y_pred_del, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_del, y_pred_del, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_test_del, y_pred_del, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_test_del, y_pred_del, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_del, y_pred_del, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_del, y_pred_del, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_del, y_pred_del, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_del, y_pred_del, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_test_del, y_pred_del, target_names=['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736d24bd",
   "metadata": {},
   "source": [
    "Here we have printed a number of performance metrics including a confusion matrix and accuracy score. However, since this is a random forest model with an imbalanced data set, other metrics may be more appropriate. \n",
    "\n",
    "Recall tells us how well our model predicted delays within a group real delays and false on-time/early flights. Precision tells us how well the model predicted delays with in a group of real dealys and false delays. The F1 score is a weighted average of both precision and recall. All three scores run on a 0 to 1 scale with 0 indicating a completely incorrect result and 1 indicating a perfect prediction. \n",
    "\n",
    "Looking at the F1 scores in the classification report, class 0, which contained the bulk of the data, had the best score which isn't surprising. Interestingly, the F1 scores for classes 1-3 were within 0.06 of one another, but the model did a better job predicting extremely long delays (0.42 - Class 4). \n",
    "\n",
    "Overall this model appears to be hampered by the imbalanced nature of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff3f69c",
   "metadata": {},
   "source": [
    "#### Undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f369b853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 47.3 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestClassifier(n_estimators = 1000, \n",
    "                            n_jobs = -1,\n",
    "                            random_state = 42)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(X_train_del_under, y_train_del_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fca513d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[ 251 2102  184   48   21]\n",
      " [ 232 7596  768  146  128]\n",
      " [ 119 3149 1347  180  162]\n",
      " [  57 1547  402  594  171]\n",
      " [  63 1180  300  154  909]]\n",
      "\n",
      "Accuracy: 0.49\n",
      "\n",
      "Micro Precision: 0.49\n",
      "Micro Recall: 0.49\n",
      "Micro F1-score: 0.49\n",
      "\n",
      "Macro Precision: 0.49\n",
      "Macro Recall: 0.36\n",
      "Macro F1-score: 0.37\n",
      "\n",
      "Weighted Precision: 0.49\n",
      "Weighted Recall: 0.49\n",
      "Weighted F1-score: 0.44\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.35      0.10      0.15      2606\n",
      "     Class 1       0.49      0.86      0.62      8870\n",
      "     Class 2       0.45      0.27      0.34      4957\n",
      "     Class 3       0.53      0.21      0.31      2771\n",
      "     Class 4       0.65      0.35      0.45      2606\n",
      "\n",
      "    accuracy                           0.49     21810\n",
      "   macro avg       0.49      0.36      0.37     21810\n",
      "weighted avg       0.49      0.49      0.44     21810\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Source: https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
    "\n",
    "# Importing confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "y_pred_del_under = rf.predict(X_test_del_under)\n",
    "\n",
    "# Define confusion matrix\n",
    "confusion = confusion_matrix(y_test_del_under, y_pred_del_under)\n",
    "\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "# Importing accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_del_under, y_pred_del_under)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_test_del_under, y_pred_del_under, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_test_del_under, y_pred_del_under, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_del_under, y_pred_del_under, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_test_del_under, y_pred_del_under, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_test_del_under, y_pred_del_under, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_del_under, y_pred_del_under, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_del_under, y_pred_del_under, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_del_under, y_pred_del_under, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_del_under, y_pred_del_under, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_test_del_under, y_pred_del_under, target_names=['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096e5878",
   "metadata": {},
   "source": [
    "Undersampling the data led to a significant drop in F1 score for class 0, and a class 1 score that was almost double what we got on the original data. However, the overall weighted average F1 score dropped from 0.53 with the original data to 0.44 with the undersampled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232027db",
   "metadata": {},
   "source": [
    "#### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "13041a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 10s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=1000, n_jobs=-1, random_state=42)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestClassifier(n_estimators = 1000, \n",
    "                            n_jobs = -1,\n",
    "                            random_state = 42)\n",
    "\n",
    "# Train the model on training data\n",
    "rf.fit(X_train_del_smote, y_train_del_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "416a083f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "\n",
      "[[17112  1982   473   167   173]\n",
      " [ 5289  9566  2293  1674  1085]\n",
      " [ 2476  1660 12815  1709  1247]\n",
      " [ 1208   744   963 16033   959]\n",
      " [ 1007   525   811  1058 16506]]\n",
      "\n",
      "Accuracy: 0.72\n",
      "\n",
      "Micro Precision: 0.72\n",
      "Micro Recall: 0.72\n",
      "Micro F1-score: 0.72\n",
      "\n",
      "Macro Precision: 0.73\n",
      "Macro Recall: 0.72\n",
      "Macro F1-score: 0.72\n",
      "\n",
      "Weighted Precision: 0.73\n",
      "Weighted Recall: 0.72\n",
      "Weighted F1-score: 0.72\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.63      0.86      0.73     19907\n",
      "     Class 1       0.66      0.48      0.56     19907\n",
      "     Class 2       0.74      0.64      0.69     19907\n",
      "     Class 3       0.78      0.81      0.79     19907\n",
      "     Class 4       0.83      0.83      0.83     19907\n",
      "\n",
      "    accuracy                           0.72     99535\n",
      "   macro avg       0.73      0.72      0.72     99535\n",
      "weighted avg       0.73      0.72      0.72     99535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Source: https://towardsdatascience.com/confusion-matrix-for-your-multi-class-machine-learning-model-ff9aa3bf7826\n",
    "\n",
    "# Importing confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Use the forest's predict method on the test data\n",
    "y_pred_del_smote = rf.predict(X_test_del_smote)\n",
    "\n",
    "# Define confusion matrix\n",
    "confusion = confusion_matrix(y_test_del_smote, y_pred_del_smote)\n",
    "\n",
    "print('Confusion Matrix\\n')\n",
    "print(confusion)\n",
    "\n",
    "# Importing accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test_del_smote, y_pred_del_smote)))\n",
    "\n",
    "print('Micro Precision: {:.2f}'.format(precision_score(y_test_del_smote, y_pred_del_smote, average='micro')))\n",
    "print('Micro Recall: {:.2f}'.format(recall_score(y_test_del_smote, y_pred_del_smote, average='micro')))\n",
    "print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test_del_smote, y_pred_del_smote, average='micro')))\n",
    "\n",
    "print('Macro Precision: {:.2f}'.format(precision_score(y_test_del_smote, y_pred_del_smote, average='macro')))\n",
    "print('Macro Recall: {:.2f}'.format(recall_score(y_test_del_smote, y_pred_del_smote, average='macro')))\n",
    "print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test_del_smote, y_pred_del_smote, average='macro')))\n",
    "\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_del_smote, y_pred_del_smote, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_del_smote, y_pred_del_smote, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_del_smote, y_pred_del_smote, average='weighted')))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('\\nClassification Report\\n')\n",
    "print(classification_report(y_test_del_smote, y_pred_del_smote, target_names=['Class 0', 'Class 1', 'Class 2', 'Class 3', 'Class 4']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d3567e",
   "metadata": {},
   "source": [
    "The oversampled data using SMOTE has the best performance of the three different splits with a weighted F1 score of 0.72. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a52bc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# #run random forest on cancellations with grid search - undersampled data\n",
    "\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# clf_del_rf = RandomForestClassifier(class_weight = 'balanced')\n",
    "\n",
    "# del_rf_params = {'n_estimators':[1000],\n",
    "#                    'max_depth': [10, 20, 100, 200],\n",
    "#                    'criterion': ['gini', 'entropy'],\n",
    "#                    'min_samples_split': [2, 5, 10],\n",
    "#                    'min_samples_leaf': [1, 2, 5, 10],\n",
    "#                    'random_state': [13]\n",
    "#                   }\n",
    "\n",
    "# del_rf_grid = GridSearchCV(estimator = clf_del_rf,\n",
    "#                                n_jobs = -1,\n",
    "#                                verbose = 1,\n",
    "#                                param_grid = del_rf_params,\n",
    "#                                cv = 10,\n",
    "#                                scoring = 'f1_weighted')\n",
    "\n",
    "# del_rf_grid.fit(X_test_del_smote,y_test_del_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a62a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #print best estimator\n",
    "# print(del_rf_grid.best_estimator_)\n",
    "\n",
    "# #with its score\n",
    "# print(\"Accuracy:\",np.abs(del_rf_grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dffce9",
   "metadata": {},
   "source": [
    "This code was commented out due to the run time it would take. To run though 18 iterations of the 960 fits took 38 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff5dad8",
   "metadata": {},
   "source": [
    "### 5.2.2 KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be888223",
   "metadata": {},
   "source": [
    "#### Original Data without Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "2c12a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://deveshpoojari.medium.com/k-nearest-neighbors-and-its-optimization-2e3f6797af04\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Creating odd list K for KNN\n",
    "neighbors = list(range(1,8,2))\n",
    "\n",
    "# Empty list that will hold cv scores\n",
    "cv_scores = [ ]\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for K in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors = K,\n",
    "                               n_jobs = -1)\n",
    "    scores = cross_val_score(knn,\n",
    "                             X_train_del,\n",
    "                             y_train_del,\n",
    "                             cv = 10,\n",
    "                             scoring = \"f1_weighted\"\n",
    "                            )\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "7115a2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal no. of neighbors is 5\n"
     ]
    }
   ],
   "source": [
    "# Changing to mis-classification error\n",
    "mse = [1-x for x in cv_scores]\n",
    "\n",
    "# Determining the best k\n",
    "optimal_k = neighbors[mse.index(min(mse))]\n",
    "print(\"The optimal no. of neighbors is {}\".format(optimal_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "0635706f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the F1 score for different values of K\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_accuracy(knn_list_scores):\n",
    "    pd.DataFrame({\"K\":[i for i in range(1,8,2)], \"F1\":knn_list_scores}).set_index(\"K\").plot.bar(rot=0)\n",
    "    plt.show()\n",
    "plot_accuracy(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d15473f",
   "metadata": {},
   "source": [
    "Performing KNN on the original delay data using 10-fold cross-validation where $K = 1, 3, 5, 7$ shows that $K = 5$ is the optimal number of neighbors for this model, returning an F1 of around 0.45."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d33f9f",
   "metadata": {},
   "source": [
    "#### Undersampled Data without Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "385377b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://deveshpoojari.medium.com/k-nearest-neighbors-and-its-optimization-2e3f6797af04\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Creating odd list K for KNN\n",
    "neighbors = list(range(1,8,2))\n",
    "\n",
    "# Empty list that will hold cv scores\n",
    "cv_scores = [ ]\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for K in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors = K,\n",
    "                               n_jobs = -1)\n",
    "    scores = cross_val_score(knn,\n",
    "                             X_train_del_under,\n",
    "                             y_train_del_under,\n",
    "                             cv = 10,\n",
    "                             scoring = \"f1_weighted\"\n",
    "                            )\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "c63ed520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal no. of neighbors is 7\n"
     ]
    }
   ],
   "source": [
    "# Changing to mis-classification error\n",
    "mse = [1-x for x in cv_scores]\n",
    "\n",
    "# Determining the best k\n",
    "optimal_k = neighbors[mse.index(min(mse))]\n",
    "print(\"The optimal no. of neighbors is {}\".format(optimal_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "a57ba021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the F1 score for different values of K\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_accuracy(knn_list_scores):\n",
    "    pd.DataFrame({\"K\":[i for i in range(1,8,2)], \"F1\":knn_list_scores}).set_index(\"K\").plot.bar(rot=0)\n",
    "    plt.show()\n",
    "plot_accuracy(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04741c16",
   "metadata": {},
   "source": [
    "On the undersampled data, KNN with 10-fold cross-validation where $K = 1, 3, 5, 7$ shows that now $K = 7$ is the optimal number of neighbors for this model, but the F1 score has dropped to ~0.29."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b60f81",
   "metadata": {},
   "source": [
    "#### SMOTE without Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "1b2bfdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://deveshpoojari.medium.com/k-nearest-neighbors-and-its-optimization-2e3f6797af04\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Creating odd list K for KNN\n",
    "neighbors = list(range(1,8,2))\n",
    "\n",
    "# Empty list that will hold cv scores\n",
    "cv_scores = [ ]\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "for K in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors = K,\n",
    "                               n_jobs = -1)\n",
    "    scores = cross_val_score(knn,\n",
    "                             X_train_del_smote,\n",
    "                             y_train_del_smote,\n",
    "                             cv = 10,\n",
    "                             scoring = \"f1_weighted\"\n",
    "                            )\n",
    "    cv_scores.append(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "9eb3d849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The optimal no. of neighbors is 1\n"
     ]
    }
   ],
   "source": [
    "# Changing to mis-classification error\n",
    "mse = [1-x for x in cv_scores]\n",
    "\n",
    "# Determining the best k\n",
    "optimal_k = neighbors[mse.index(min(mse))]\n",
    "print(\"The optimal no. of neighbors is {}\".format(optimal_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "4ef66952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEJCAYAAACAKgxxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXvUlEQVR4nO3dfXBU5aHH8d+G8GIEDYTdxFtnfLnMWGoTin0hTTvJaCErkYVow5QLQ7xio1iZTDMdlAEKlA6KOBi1rZRkWrlekmuoA4FtOyFVWmfapNV4ZwwiL0Wut1Jld8mKEFwg4Zz7h9PtXYGc3XiSZR++n7949jy7+e1D+OVwsuccj23btgAAxshKdwAAgLsodgAwDMUOAIah2AHAMBQ7ABiGYgcAw1DsAGCY7HQHkKQPPzwty7q8P06flzdWPT296Y5hDNbTPayluzJhPbOyPBo//upLbr8sit2y7Mu+2CVlRMZMwnq6h7V0V6avJ4diAMAwFDsAGOayOBQDAG6xbVsffhjRuXNnJKV+SCUczpJlWe4HS5lHo0aN0fjxXnk8npSeSbEDMEpv70fyeDzKz79eHk/qByWys7PU35/+YrdtSydOHFdv70caNy43pedyKAaAUWKxXo0blzuoUr+ceDxZGjduvGKx1D+hk9nvHAA+xbLOa8QIMw5GjBiRLcs6n/LzKHYAxkn1mPTlarDvw4wfa58y7pqrNGa0+2/N6x3n6uudOduvUydjrr4mgERD1QfJ/vv94IP39W//do9uvPHmhMefeOIp5ecX6PXX/6ytW/9DzzyzybVsSb3bYDCoTZs2qb+/X/fee68WLFiQsH3fvn1atWqV+vr6dN111+nJJ5/UNddc41rIVI0Zna3AD3am7esnK7hxjk6lOwRguKHqg1T+/U6c6NWWLc0Jj1mWpf/6r636z/98Xjff/K+uZnM8FBMKhVRfX6/m5ma1traqpaVFhw8fTpizbt061dbWateuXbrpppv0i1/8wtWQAGCad9/9H/3v//6PHn10heuv7bjH3tHRoeLiYuXm5kqS/H6/2tratGTJkvgcy7J0+vRpSVIsFtO1117relAAyFTHj0f07/8+Pz4uL79T8+dXa9myH+q//7vL9a/nWOzhcFherzc+9vl86u7uTpizbNkyLVq0SI899piuuuoqbdu2zfWgAJCpLnYoZig5FrtlWQm/mbVtO2F85swZrVixQlu2bFFRUZGef/55Pfroo2poaEg6RF7e2BRjm8PtX8hmkiv5vbuNtfyncDhL2dnD84G/ZL7OiBFZA84dMSJLHo/nktuzsrJS/vt1LPaCggJ1df3zvwqRSEQ+ny8+PnTokEaPHq2ioiJJ0ne+8x0988wzKYXo6el19WpqmfRNHolcmb8+9XrHXbHv3W2sZSLLsobtzNFkvs7589aAc8+ft2Tb9iW3W5Z1wd9vVpZnwB1ixx83JSUl6uzsVDQaVSwWU3t7u0pLS+Pbb7jhBh07dkxHjhyRJL3yyisqLCx0elkAwBBx3GPPz89XXV2dqqur1dfXp6qqKhUVFammpka1tbUqLCzU448/ru9///uybVt5eXl67LHHhiM7ADg6c7ZfwY1zhuR1k3Hddf+il14KXnL7bbd9Rbfd9hW3YklK8nPsgUBAgUAg4bHGxsb4n8vKylRWVuZqMABww6mTsZTOF7lcLgL2WXBJAQAwDMUOAIah2AEYx7Yz+56l/zDY90GxAzBKdvYonT59MuPL3bZtnT59UtnZo1J+rpFXdwRw5Ro/3qsPP4yot/fEoJ6flXW53Brvkx9S48d7nSd++nlDkAUA0mbEiGxNnHjdoJ9vwglfFDsccX17ILNQ7HDE9e2BzMIvTwHAMBQ7ABiGYgcAw1DsAGAYih0ADEOxA4BhKHYAMAzFDgCGodgBwDBJnXkaDAa1adMm9ff3695779WCBQvi2/bv369ly5bFx9FoVNdee61+/etfu58WAODIsdhDoZDq6+u1fft2jRo1SvPmzdO0adM0adIkSdLkyZO1c+cnp5vHYjHNnTtXa9asGdLQQKbiujsYDo7fYR0dHSouLlZubq4kye/3q62tTUuWLLlg7ubNm/XVr35VX/mKuzdmBUzBdXcwHByLPRwOy+v95/WAfT6furu7L5h36tQpbdu2TcHgpe/GfSl5eWNTfo4p3N7TutKxnu65ktcy09+7Y7FbliWPxxMf27adMP6HXbt2afr06crLy0s5RE9PryzLvbudZNJfSiZc95n1dA9refnLhOuxZ2V5BtwhdvxUTEFBgSKRSHwciUTk8/kumPfyyy+roqJikDEBAG5xLPaSkhJ1dnYqGo0qFoupvb1dpaWlCXNs29a+ffs0derUIQsKAEiOY7Hn5+errq5O1dXVqqys1KxZs1RUVKSamhrt3btX0icfcRw5cqRGjx495IEBAANL6nNXgUBAgUAg4bHGxsb4n/Py8vSnP/3J3WQAgEHhzFMAMAz3PAWQsTjh6+IodgAZixO+Lo5DMQBgGIodAAxDsQOAYSh2ADAMxQ4AhqHYAcAwFDsAGIZiBwDDUOwAYBiKHQAMQ7EDgGEodgAwDMUOAIZJqtiDwaAqKipUXl6upqamC7YfOXJECxcu1OzZs3X//ffro48+cj0oACA5jsUeCoVUX1+v5uZmtba2qqWlRYcPH45vt21bDz30kGpqarRr1y5NnjxZDQ0NQxoaAHBpjsXe0dGh4uJi5ebmKicnR36/X21tbfHt+/btU05OTvwG14sXL9aCBQuGLjEAYECOxR4Oh+X1euNjn8+nUCgUH//tb3/TxIkTtXz5ct19991avXq1cnJyhiYtAMCR4x2ULMuSx+OJj23bThj39/frtdde09atW1VYWKinn35a69ev1/r165MOkZc3NsXY5nD7FlxXOtbTPaylu4ZzPR2LvaCgQF1dXfFxJBKRz+eLj71er2644QYVFhZKkmbNmqXa2tqUQvT09Mqy7JSeM5BM+oaMRIbzhlmDw3q6h7V015W6nllZngF3iB0PxZSUlKizs1PRaFSxWEzt7e3x4+mSNHXqVEWjUR04cECStGfPHt16660uRAcADIbjHnt+fr7q6upUXV2tvr4+VVVVqaioSDU1NaqtrVVhYaF+9rOfaeXKlYrFYiooKNCGDRuGIzsA4CIci12SAoGAAoFAwmONjY3xP0+ZMkUvvfSSu8kAAIPCmacAYBiKHQAMQ7EDgGEodgAwDMUOAIah2AHAMBQ7ABiGYgcAw1DsAGAYih0ADEOxA4BhKHYAMAzFDgCGodgBwDAUOwAYhmIHAMMkVezBYFAVFRUqLy9XU1PTBdt/+tOf6vbbb9ecOXM0Z86ci84BAAwPxzsohUIh1dfXa/v27Ro1apTmzZunadOmadKkSfE5b731lp566ilNnTp1SMMCAJw57rF3dHSouLhYubm5ysnJkd/vV1tbW8Kct956S5s3b1YgENDatWt19uzZIQsMABiYY7GHw2F5vd742OfzKRQKxcenT5/W5MmTtXTpUu3YsUMnT57Uc889NzRpAQCOHA/FWJYlj8cTH9u2nTC++uqrE25svWjRIi1fvlx1dXVJh8jLG5v0XNN4vePSHcEorKd7WEt3Ded6OhZ7QUGBurq64uNIJCKfzxcfv//+++ro6FBVVZWkT4o/O9vxZRP09PTKsuyUnjOQTPqGjEROpTuCI9bTPaylu67U9czK8gy4Q+x4KKakpESdnZ2KRqOKxWJqb29XaWlpfPuYMWP05JNP6r333pNt22pqatKMGTPcSQ8ASJljsefn56uurk7V1dWqrKzUrFmzVFRUpJqaGu3du1cTJkzQ2rVr9dBDD+nOO++Ubdu67777hiM7AOAikjpmEggEFAgEEh77/8fV/X6//H6/u8kAAIPCmacAYBiKHQAMQ7EDgGEodgAwDMUOAIah2AHAMBQ7ABiGYgcAw1DsAGAYih0ADEOxA4BhKHYAMAzFDgCGodgBwDAUOwAYhmIHAMNQ7ABgmKSKPRgMqqKiQuXl5WpqarrkvD/84Q+64447XAsHAEid463xQqGQ6uvrtX37do0aNUrz5s3TtGnTNGnSpIR5x48f1xNPPDFkQQEAyXHcY+/o6FBxcbFyc3OVk5Mjv9+vtra2C+atXLlSS5YsGZKQAIDkOe6xh8Nheb3e+Njn86m7uzthzgsvvKAvfOELmjJlyqBC5OWNHdTzTOD1jkt3BKOwnu5hLd01nOvpWOyWZcnj8cTHtm0njA8dOqT29nZt2bJFx44dG1SInp5eWZY9qOdeTCZ9Q0Yip9IdwRHr6R7W0l1X6npmZXkG3CF2PBRTUFCgSCQSH0ciEfl8vvi4ra1NkUhE3/72t/XAAw8oHA5r/vz5nzE2AGCwHIu9pKREnZ2dikajisViam9vV2lpaXx7bW2tdu/erZ07d6qhoUE+n0/Nzc1DGhoAcGmOxZ6fn6+6ujpVV1ersrJSs2bNUlFRkWpqarR3797hyAgASIHjMXZJCgQCCgQCCY81NjZeMO/666/Xnj173EkGABgUzjwFAMNQ7ABgGIodAAxDsQOAYSh2ADAMxQ4AhqHYAcAwFDsAGIZiBwDDUOwAYBiKHQAMQ7EDgGEodgAwDMUOAIah2AHAMBQ7ABgmqWIPBoOqqKhQeXm5mpqaLtj+u9/9ToFAQHfddZeWLVumc+fOuR4UAJAcx2IPhUKqr69Xc3OzWltb1dLSosOHD8e3f/zxx1q7dq2ef/55/eY3v9HZs2e1Y8eOIQ0NALg0x2Lv6OhQcXGxcnNzlZOTI7/fr7a2tvj2nJwc7dmzRxMnTlQsFlNPT4+uueaaIQ0NALg0x3uehsNheb3e+Njn86m7uzthzsiRI/Xqq6/qkUcekc/n0ze/+c2UQuTljU1pvkm83nHpjmAU1tM9rKW7hnM9HYvdsix5PJ742LbthPE/lJWV6S9/+YueeuoprVmzRhs3bkw6RE9PryzLTnq+k0z6hoxETqU7giPW0z2spbuu1PXMyvIMuEPseCimoKBAkUgkPo5EIvL5fPHxiRMn9Mc//jE+DgQCOnjw4GDzAgA+I8diLykpUWdnp6LRqGKxmNrb21VaWhrfbtu2li5dqvfff1+S1NbWpttuu23oEgMABuR4KCY/P191dXWqrq5WX1+fqqqqVFRUpJqaGtXW1qqwsFA//vGP9eCDD8rj8WjSpEn60Y9+NBzZAQAX4Vjs0ieHVwKBQMJjjY2N8T9Pnz5d06dPdzcZAGBQOPMUAAxDsQOAYSh2ADAMxQ4AhqHYAcAwFDsAGIZiBwDDUOwAYBiKHQAMQ7EDgGEodgAwDMUOAIah2AHAMBQ7ABiGYgcAw1DsAGCYpIo9GAyqoqJC5eXlampqumD7yy+/rDlz5mj27Nn63ve+p48++sj1oACA5DgWeygUUn19vZqbm9Xa2qqWlhYdPnw4vr23t1dr1qxRQ0ODdu3apVtuuUU/+clPhjQ0AODSHIu9o6NDxcXFys3NVU5Ojvx+v9ra2uLb+/r6tHr1auXn50uSbrnlFn3wwQdDlxgAMCDHYg+Hw/J6vfGxz+dTKBSKj8ePH68ZM2ZIks6cOaOGhgbufwoAaeR4M2vLsuTxeOJj27YTxv9w6tQpPfzww/r85z+vu+++O6UQeXljU5pvEq93XLojGIX1dA9r6a7hXE/HYi8oKFBXV1d8HIlE5PP5EuaEw2Hdf//9Ki4u1vLly1MO0dPTK8uyU37epWTSN2QkcirdERyxnu5hLd11pa5nVpZnwB1ix0MxJSUl6uzsVDQaVSwWU3t7u0pLS+Pbz58/r8WLF2vmzJlasWLFRffmAQDDx3GPPT8/X3V1daqurlZfX5+qqqpUVFSkmpoa1dbW6tixY3r77bd1/vx57d69W5L0xS9+UevWrRvy8ACACzkWuyQFAgEFAoGExxobGyVJhYWFOnDggPvJAACDwpmnAGAYih0ADEOxA4BhKHYAMAzFDgCGodgBwDAUOwAYhmIHAMNQ7ABgGIodAAxDsQOAYSh2ADAMxQ4AhqHYAcAwFDsAGIZiBwDDJFXswWBQFRUVKi8vV1NT0yXnPfLII9q+fbtr4QAAqXMs9lAopPr6ejU3N6u1tVUtLS06fPjwBXMWL14cvzUeACB9HIu9o6NDxcXFys3NVU5Ojvx+v9ra2hLmBINBfetb39LMmTOHLCgAIDmO9zwNh8Pyer3xsc/nU3d3d8Kc7373u5KkN954w+V4AIBUORa7ZVnyeDzxsW3bCWM35OWNdfX1MonXOy7dEYzCerqHtXTXcK6nY7EXFBSoq6srPo5EIvL5fK6G6OnplWXZrr1eJn1DRiKn0h3BEevpHtbSXVfqemZleQbcIXY8xl5SUqLOzk5Fo1HFYjG1t7ertLTUtYAAAHc5Fnt+fr7q6upUXV2tyspKzZo1S0VFRaqpqdHevXuHIyMAIAWOh2IkKRAIKBAIJDzW2Nh4wbz169e7kwoAMGiceQoAhqHYAcAwFDsAGIZiBwDDUOwAYBiKHQAMQ7EDgGEodgAwDMUOAIah2AHAMBQ7ABiGYgcAw1DsAGAYih0ADEOxA4BhKHYAMAzFDgCGSarYg8GgKioqVF5erqampgu279+/X/fcc4/8fr9WrFih/v5+14MCAJLjWOyhUEj19fVqbm5Wa2urWlpadPjw4YQ5S5cu1apVq7R7927Ztq1t27YNWWAAwMAc73na0dGh4uJi5ebmSpL8fr/a2tq0ZMkSSdLf//53nTlzRl/60pckSffcc4+effZZzZ8/P+kQWVme1JM78I2/yvXXHApD8d6HAuvpHtbSXVfiejq9lmOxh8Nheb3e+Njn86m7u/uS271er0KhUEohx4+/OqX5yfjFynLXX3Mo5OWNTXeEpLCe7mEt3cV6XsjxUIxlWfJ4/vnTwbbthLHTdgDA8HIs9oKCAkUikfg4EonI5/Ndcvvx48cTtgMAhpdjsZeUlKizs1PRaFSxWEzt7e0qLS2Nb//c5z6n0aNH64033pAk7dy5M2E7AGB4eWzbtp0mBYNBbd68WX19faqqqlJNTY1qampUW1urwsJCHThwQCtXrlRvb69uvfVWPf744xo1atRw5AcAfEpSxQ4AyByceQoAhqHYAcAwFDsAGIZiBwDDOJ55Cqm3t1fz5s3Tz3/+c11//fXpjpPRnnnmGe3evVsej0dVVVW677770h0poy1cuFDRaFTZ2Z/8U167dq2mTJmS5lSZ51e/+pW2bt0aHx89elRz5szRqlWr0phq8Ch2B2+++aZWrlypd999N91RMt5rr72mP//5z9q1a5f6+/tVUVGhsrIy3XzzzemOlpFs29a7776r3//+9/Fix+DMnTtXc+fOlST99a9/1cMPPxy/HlYm4lCMg23btmn16tWcTeuCr33ta3rhhReUnZ2tnp4enT9/Xjk5OemOlbGOHDkiSVq0aJFmz56dsMeJwVuzZo3q6uo0YcKEdEcZNH7MO1i3bl26Ixhl5MiRevbZZ/XLX/5Sd955p/Lz89MdKWOdPHlSX//61/XDH/5QfX19qq6u1k033aRvfOMb6Y6WsTo6OnTmzBnNnDkz3VE+E/bYMexqa2vV2dmpDz74gGv3fwZTp07Vhg0bNG7cOE2YMEFVVVV69dVX0x0ro7344otG/N6HYseweeedd7R//35J0lVXXaXy8nIdPHgwzakyV1dXlzo7O+Nj27Y51v4ZnDt3Tq+//rruuOOOdEf5zCh2DJujR49q5cqVOnfunM6dO6dXXnlFX/7yl9MdK2OdOnVKGzZs0NmzZ9Xb26sdO3ZoxowZ6Y6VsQ4ePKgbb7zRiN/78OMdw6asrEzd3d2qrKzUiBEjVF5errvuuivdsTLW7bffrjfffFOVlZWyLEvz58/X1KlT0x0rY7333nsqKChIdwxXcBEwADAMh2IAwDAUOwAYhmIHAMNQ7ABgGIodAAxDsQP/z9GjRy/4yOBvf/tbTZs2LeFkIOByxufYgQG8+OKLeu6557RlyxZNnjw53XGApFDswCU0NDRo+/btam5u5jr8yCgcigEuYsOGDdq4caMWLlxIqSPjUOzAp3z88cc6dOiQGhoatHHjRr399tvpjgSkhGIHPmXMmDHatGmTysrK9OCDD2rJkiU6ceJEumMBSaPYgU/JysrSyJEjJUkPPPCAJk2apB/84AeyLCvNyYDkUOzAADwej5544gm98847evrpp9MdB0gKV3cEAMOwxw4AhqHYAcAwFDsAGIZiBwDDUOwAYBiKHQAMQ7EDgGEodgAwzP8B0ffxzl1yP54AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the F1 score for different values of K\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_accuracy(knn_list_scores):\n",
    "    pd.DataFrame({\"K\":[i for i in range(1,8,2)], \"F1\":knn_list_scores}).set_index(\"K\").plot.bar(rot=0)\n",
    "    plt.show()\n",
    "plot_accuracy(cv_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad5e3c0",
   "metadata": {},
   "source": [
    "Interestingly, running KNN with the same cross-validation and number of $Kâ€™s$, we find that $K = 1$ is the optimal number with an F1 score of slightly above 0.8. This is a similar performance result from SMOTE that we saw with the random forest models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46176dc7",
   "metadata": {},
   "source": [
    "#### SMOTE with Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "9ec3b0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 10min 25s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 1, 'weights': 'uniform'}"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# # https://realpython.com/knn-python/\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# #Create KNN Classifier\n",
    "parameters = {\n",
    "     \"n_neighbors\": list(range(1,20,2)),\n",
    "     \"weights\": [\"uniform\", \"distance\"],\n",
    " }\n",
    "gridsearch = GridSearchCV(KNeighborsClassifier(), parameters, cv=10, scoring = 'f1_weighted')\n",
    "gridsearch.fit(X_train_del_smote, y_train_del_smote)\n",
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e808790a",
   "metadata": {},
   "source": [
    "After running a grid search to help select the best parameters, `K = 1` and the `weights = 'uniform'` are chosen for the KNN classfier model using SMOTE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "186d2d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds_grid2 = gridsearch.predict(X_test_del_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "c047857c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8439989486944779\n",
      "Weighted Precision: 0.84\n",
      "Weighted Recall: 0.84\n",
      "Weighted F1-score: 0.83\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test_del_smote, test_preds_grid2))\n",
    "print('Weighted Precision: {:.2f}'.format(precision_score(y_test_del_smote, test_preds_grid2, average='weighted')))\n",
    "print('Weighted Recall: {:.2f}'.format(recall_score(y_test_del_smote, test_preds_grid2, average='weighted')))\n",
    "print('Weighted F1-score: {:.2f}'.format(f1_score(y_test_del_smote, test_preds_grid2, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37bd428",
   "metadata": {},
   "source": [
    "After running a grid search on the KNN model with SMOTE, this was the best performing model with the lowest amount of neighbors at 1. This performed well with all of the evaluation metrics that were selected in our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "b85f12d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save and run model with K=1 and pull metrics\n",
    "knn_delay = KNeighborsClassifier(n_neighbors=1, weights='uniform')\n",
    "knn_delay.fit(X_train_del_smote, y_train_del_smote)\n",
    "y_pred_knn_del = knn_delay.predict(X_test_del_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27598c80",
   "metadata": {},
   "source": [
    "### 5.2.3 SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d003e82e",
   "metadata": {},
   "source": [
    "#### Original Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "b02f677d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   28.0s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  1.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 30s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=SGDClassifier(class_weight='balanced',\n",
       "                                     early_stopping=True),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'alpha': [0.0001, 0.001, 0.01],\n",
       "                         'penalty': ['l2', 'l1', 'elasticnet'],\n",
       "                         'random_state': [42]},\n",
       "             scoring='f1_weighted', verbose=1)"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "#Create SVM Classifier\n",
    "del_svm_params = {\n",
    "    \"penalty\": ['l2', 'l1', 'elasticnet'],\n",
    "    \"alpha\": [0.0001, 0.001, 0.01],\n",
    "    \"random_state\": [42]\n",
    "}\n",
    "\n",
    "del_svm_grid0 = GridSearchCV(estimator = SGDClassifier(early_stopping=True, class_weight = 'balanced'),\n",
    "                               n_jobs = -1,\n",
    "                               verbose = 1,\n",
    "                               param_grid = del_svm_params,\n",
    "                               cv = 10,\n",
    "                               scoring = 'f1_weighted')\n",
    "\n",
    "del_svm_grid0.fit(X_train_del, y_train_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "60221205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=0.001, class_weight='balanced', early_stopping=True,\n",
      "              penalty='l1', random_state=42)\n",
      "F1_weighted: 0.19263205910002779\n"
     ]
    }
   ],
   "source": [
    "#print best estimator\n",
    "print(del_svm_grid0.best_estimator_)\n",
    "\n",
    "#with its score\n",
    "print(\"F1_weighted:\",np.abs(del_svm_grid0.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0fdf21",
   "metadata": {},
   "source": [
    "#### Undersample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "2a1ea611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:   41.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 43 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=SGDClassifier(class_weight='balanced',\n",
       "                                     early_stopping=True),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'alpha': [0.0001, 0.001, 0.01],\n",
       "                         'penalty': ['l2', 'l1', 'elasticnet'],\n",
       "                         'random_state': [42]},\n",
       "             scoring='f1_weighted', verbose=1)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "#Create SVM Classifier\n",
    "del_svm_params = {\n",
    "    \"penalty\": ['l2', 'l1', 'elasticnet'],\n",
    "    \"alpha\": [0.0001, 0.001, 0.01],\n",
    "    \"random_state\": [42]\n",
    "}\n",
    "\n",
    "del_svm_grid1 = GridSearchCV(estimator = SGDClassifier(early_stopping=True, class_weight = 'balanced'),\n",
    "                               n_jobs = -1,\n",
    "                               verbose = 1,\n",
    "                               param_grid = del_svm_params,\n",
    "                               cv = 10,\n",
    "                               scoring = 'f1_weighted')\n",
    "\n",
    "del_svm_grid1.fit(X_train_del_under, y_train_del_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "d4e97d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(class_weight='balanced', early_stopping=True, random_state=42)\n",
      "Accuracy: 0.16431898290636254\n"
     ]
    }
   ],
   "source": [
    "#print best estimator\n",
    "print(del_svm_grid1.best_estimator_)\n",
    "\n",
    "#with its score\n",
    "print(\"F1_weighted:\",np.abs(del_svm_grid1.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c94fc9",
   "metadata": {},
   "source": [
    "#### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "ddd0a15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  4.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 39s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10,\n",
       "             estimator=SGDClassifier(class_weight='balanced',\n",
       "                                     early_stopping=True),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'alpha': [0.0001, 0.001, 0.01],\n",
       "                         'penalty': ['l2', 'l1', 'elasticnet'],\n",
       "                         'random_state': [42]},\n",
       "             scoring='f1_weighted', verbose=1)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "#Create SVM Classifier\n",
    "del_svm_params = {\n",
    "    \"penalty\": ['l2', 'l1', 'elasticnet'],\n",
    "    \"alpha\": [0.0001, 0.001, 0.01],\n",
    "    \"random_state\": [42]\n",
    "}\n",
    "\n",
    "del_svm_grid2 = GridSearchCV(estimator = SGDClassifier(early_stopping=True, class_weight = 'balanced'),\n",
    "                               n_jobs = -1,\n",
    "                               verbose = 1,\n",
    "                               param_grid = del_svm_params,\n",
    "                               cv = 10,\n",
    "                               scoring = 'f1_weighted')\n",
    "\n",
    "del_svm_grid2.fit(X_train_del_smote,y_train_del_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "637a9fc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=0.001, class_weight='balanced', early_stopping=True,\n",
      "              penalty='l1', random_state=42)\n",
      "F1_weighted: 0.17970411243679513\n"
     ]
    }
   ],
   "source": [
    "#print best estimator\n",
    "print(del_svm_grid2.best_estimator_)\n",
    "\n",
    "#with its score\n",
    "print(\"F1_weighted:\",np.abs(del_svm_grid2.best_score_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10af6fa1",
   "metadata": {},
   "source": [
    "Similar to our cancellation modelling, SGDClassifier does not perform well on the delay data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b080892",
   "metadata": {},
   "source": [
    "## 6. Modeling and Evaluation 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f76b39",
   "metadata": {},
   "source": [
    "### 6.2 Delay Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb1f7bb",
   "metadata": {},
   "source": [
    "| Model Type | Data set| Test F1 | Precision | Recall\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Random Forest | Orginal | 0.53 | 0.55 | 0.58\n",
    "| Random Forest | Undersampled | 0.44 | 0.49 | 0.49\n",
    "| Random Forest | SMOTE | 0.72 | 0.72 | 0.72\n",
    "| KNN | SMOTE | 0.83 | 0.84 | 0.84\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fe340e",
   "metadata": {},
   "source": [
    "Our best performing model types were random forest and KNN using the SMOTE split. Of these, the best in terms of F1 score, precision, and recall was the KNN model with an F1 score on the test set of 0.83. We will visualize some of these results via confusion matrices below, where we would expect a good performing model to have the darkest blue colors along the diagonal. This indicates a higher accuracy in terms of predicted class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472c81ed",
   "metadata": {},
   "source": [
    "### 7.2 Delay Data Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e3b75",
   "metadata": {},
   "source": [
    "For our delay data, while KNN with SMOTE is our best performing model, we chose to focus on Random Forest with SMOTE for feature importance as we do not have coefficients or importances from KNN. The features most important are below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "6d972782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TAIL_NUMBER_encode            0.076024\n",
       "FLIGHT_NUMBER                 0.070683\n",
       "TAXI_OUT                      0.065760\n",
       "SCHEDULED_TIME                0.065471\n",
       "DESTINATION_AIRPORT_encode    0.061690\n",
       "dtype: float64"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature important for RF on SMOTE data - delays\n",
    "\n",
    "features_del = df_delay.iloc[:,:-1]\n",
    "\n",
    "feature_imp_del = pd.Series(rf.feature_importances_,index=features_del.columns).sort_values(ascending=False)\n",
    "#show top 5\n",
    "feature_imp_del[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4252c7ef",
   "metadata": {},
   "source": [
    "In terms of delay, weather did not make the top five important features. Instead, `TAIL_NUMBER_encode` (which is our encoded variable for the plane) showed as the most important followed closely by `FLIGHT_NUMBER`. Typically, one plane will fly the same route multiple times which is why it would make sense that the `FLIGHT_NUMBER` would be almost as important as the plane flying it. `TAXI_OUT` suggests congestion at the airport for take off which would lead to departure delays. `SCHEDULED_TIME` and `DESTINATION_AIRPORT_encode` suggest that based on destination, a departure delay is more likely to occur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9739dd6",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "Jump to [top](#Rubric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f66b83",
   "metadata": {},
   "source": [
    "# Exceptional Work\n",
    "Jump to [top](#Rubric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
